const fs = require('fs').promises;
const path = require('path');
const crypto = require('crypto');
const sharp = require('sharp');
const { getInstance: getLlamaService } = require('../services/LlamaService');
const { globalDeduplicator } = require('../utils/llmOptimization');
const { extractAndParseJSON } = require('../utils/jsonRepair');
const { attemptJsonRepairWithLlama } = require('../utils/llmJsonRepair');
const { AI_DEFAULTS, SUPPORTED_IMAGE_EXTENSIONS } = require('../../shared/constants');
const { TRUNCATION, TIMEOUTS } = require('../../shared/performanceConstants');
const { withAbortableTimeout } = require('../../shared/promiseUtils');
const { ANALYSIS_SCHEMA_PROMPT } = require('../../shared/analysisSchema');
const { analyzeTextWithLlama } = require('./documentLlm');
const { recognizeIfAvailable } = require('../utils/tesseractUtils');
const { normalizeAnalysisResult } = require('./utils');
const { normalizeExtractedTextForStorage } = require('./analysisTextUtils');
const {
  getIntelligentCategory: getIntelligentImageCategory,
  getIntelligentKeywords: getIntelligentImageKeywords,
  safeSuggestedName,
  createFallbackAnalysis
} = require('./fallbackUtils');
const FolderMatchingService = require('../services/FolderMatchingService');
const { analysisQueue } = require('./embeddingQueue/stageQueues');
const embeddingQueueManager = require('./embeddingQueue/queueManager');
const { createLogger } = require('../../shared/logger');
const { findContainingSmartFolder } = require('../../shared/folderUtils');
const { getSemanticFileId } = require('../../shared/fileIdUtils');
const { shouldEmbed } = require('../services/embedding/embeddingGate');
const {
  applySemanticFolderMatching: applyUnifiedFolderMatching,
  getServices
} = require('./semanticFolderMatcher');
const { getImageAnalysisCache } = require('../services/AnalysisCacheService');

const logger = createLogger('ImageAnalysis');
const IMAGE_SIGNATURE_VERSION = 'v2';

/**
 * Set image cache value with automatic TTL and LRU eviction
 * @param {string} signature - Cache key
 * @param {Object} value - Value to cache
 */
function setImageCache(signature, value) {
  if (!signature) return;
  getImageAnalysisCache().set(signature, value);
}

/**
 * Create a vision-unavailable fallback analysis with optional extracted text.
 * Consolidates the repeated pattern of building a fallback + attaching text.
 *
 * @param {Object} params - Parameters matching createFallbackAnalysis
 * @param {string|null} extractedText - Raw extracted text (will be normalized)
 * @returns {Object} Fallback analysis result
 */
function createVisionUnavailableFallback(params, extractedText) {
  const fallback = createFallbackAnalysis(params);
  const extractedTextForStorage = normalizeExtractedTextForStorage(extractedText);
  if (extractedTextForStorage) {
    fallback.extractedText = extractedTextForStorage;
  }
  return fallback;
}

// App configuration for image analysis - Optimized for speed
const AppConfig = {
  ai: {
    imageAnalysis: {
      defaultModel: AI_DEFAULTS?.IMAGE?.MODEL || 'llava-v1.6-mistral-7b-Q4_K_M.gguf',
      // Keep image analysis aligned with global processing timeouts so the renderer lock
      // doesn't get "stuck" for minutes when vision calls hang.
      timeout: TIMEOUTS.AI_ANALYSIS_LONG,
      temperature: AI_DEFAULTS?.IMAGE?.TEMPERATURE || 0.2,
      maxTokens: AI_DEFAULTS?.IMAGE?.MAX_TOKENS || 512
    }
  }
};

const OCR_DEFAULTS = {
  timeoutMs: TIMEOUTS.AI_ANALYSIS_MEDIUM,
  maxTokens: 400,
  maxRetries: 1
};

const OCR_FILENAME_HINTS = [
  'report',
  'document',
  'invoice',
  'receipt',
  'form',
  'screenshot',
  'screen',
  'budget',
  'financial',
  'statement',
  'tax'
];
const OCR_GROUNDING_MAX_CHARS = 600;
const OCR_POST_PASS_CONFIDENCE_SKIP_THRESHOLD = 88;
const OCR_POST_PASS_STRICT_SKIP_THRESHOLD = 92;
const IMAGE_PREFLIGHT_TTL_MS = Number(process.env.STRATOSORT_IMAGE_PREFLIGHT_TTL_MS) || 15000;

const MODEL_NOT_AVAILABLE_PATTERNS = [
  'model not found',
  'model does not exist',
  'not loaded',
  'unknown model',
  'no such file',
  'mmproj',
  'projector',
  'vision runtime',
  'llama-server',
  'llama server',
  'runtime failed',
  'failed to start'
];

function stripJsonCodeFence(text) {
  if (typeof text !== 'string') return text;
  const trimmed = text.trim();
  const fencedMatch = trimmed.match(/^```(?:json)?\s*([\s\S]*?)\s*```$/i);
  return fencedMatch ? fencedMatch[1].trim() : text;
}

function isModelNotAvailableError(errorLike) {
  const message = String(errorLike?.message || errorLike || '').toLowerCase();
  return MODEL_NOT_AVAILABLE_PATTERNS.some((pattern) => message.includes(pattern));
}

function hasTextNameHint(fileNameLower) {
  return OCR_FILENAME_HINTS.some((hint) => fileNameLower.includes(hint));
}

function isRecoverableVisionResultError(errorLike) {
  const message =
    typeof errorLike === 'string'
      ? errorLike.toLowerCase()
      : String(errorLike?.message || errorLike || '').toLowerCase();
  if (!message) return false;
  return (
    message.includes('timeout') ||
    message.includes('timed out') ||
    message.includes('empty response') ||
    message.includes('no response') ||
    message.includes('failed to parse') ||
    message.includes('json') ||
    message.includes('undefined result')
  );
}

function shouldRunPostVisionOcr(analysis, hasTextHint) {
  if (!analysis || analysis.error) return false;

  const contentType = String(analysis.content_type || '').toLowerCase();
  const contentSuggestsText =
    contentType.includes('text') ||
    contentType.includes('document') ||
    contentType.includes('screenshot');
  const hasTextSignal = analysis.has_text === true || contentSuggestsText;
  if (!hasTextSignal) return false;

  const confidence = Number(analysis.confidence) || 0;
  const hasSuggestedName =
    typeof analysis.suggestedName === 'string' && analysis.suggestedName.trim().length > 0;
  const keywordCount = Array.isArray(analysis.keywords)
    ? analysis.keywords.filter(Boolean).length
    : 0;
  const missingCoreFields = !hasSuggestedName || keywordCount < 2;

  if (confidence >= OCR_POST_PASS_STRICT_SKIP_THRESHOLD && !missingCoreFields) return false;
  if (!hasTextHint && confidence >= OCR_POST_PASS_CONFIDENCE_SKIP_THRESHOLD && !missingCoreFields) {
    return false;
  }

  return true;
}

const preflightCache = {
  value: null,
  expiresAt: 0,
  pending: null,
  serviceRef: null
};

async function getImagePreflight(llamaService, { forceRefresh = false } = {}) {
  const now = Date.now();
  const sameService = preflightCache.serviceRef === llamaService;
  if (!forceRefresh && sameService && preflightCache.value && preflightCache.expiresAt > now) {
    return preflightCache.value;
  }
  if (!forceRefresh && sameService && preflightCache.pending) {
    return preflightCache.pending;
  }

  preflightCache.pending = (async () => {
    let health = { success: true, status: 'healthy' };
    if (typeof llamaService?.testConnection === 'function') {
      try {
        health = await llamaService.testConnection();
      } catch (error) {
        health = { success: false, status: 'error', error: error?.message || 'preflight_failed' };
      }
    }
    const isHealthy = Boolean(health?.success) || health?.status === 'healthy';
    const cfg =
      typeof llamaService?.getConfig === 'function'
        ? await Promise.resolve(llamaService.getConfig()).catch(() => ({}))
        : {};
    const availableModels =
      typeof llamaService?.listModels === 'function'
        ? await Promise.resolve(llamaService.listModels()).catch(() => [])
        : [];
    const modelNames = Array.isArray(availableModels)
      ? availableModels.map((m) => m.name || m.filename || '')
      : [];
    const preflight = {
      health,
      isHealthy,
      cfg,
      modelNames,
      visionModelName:
        cfg?.visionModel ||
        AppConfig.ai.imageAnalysis.defaultModel ||
        'llava-v1.6-mistral-7b-Q4_K_M.gguf'
    };
    preflightCache.serviceRef = llamaService;
    preflightCache.value = preflight;
    preflightCache.expiresAt = Date.now() + IMAGE_PREFLIGHT_TTL_MS;
    return preflight;
  })().finally(() => {
    preflightCache.pending = null;
  });

  return preflightCache.pending;
}

// JSON repair constants and function consolidated to ../utils/llmJsonRepair.js
const IMAGE_ANALYSIS_SCHEMA = {
  ...ANALYSIS_SCHEMA_PROMPT,
  colors: ['#hex1', '#hex2'],
  has_text: true,
  content_type: 'text_document OR photograph OR screenshot OR other'
};
// Local JSON repair removed - using ../utils/llmJsonRepair.js

async function analyzeImageWithLlama(
  imageBase64,
  originalFileName,
  smartFolders = [],
  extractedText = null,
  namingContext = [],
  options = {}
) {
  const bypassDedup = Boolean(options?.bypassCache);
  try {
    const startedAt = Date.now();
    logger.info(`Analyzing image content with AI engine`, {
      model: AppConfig.ai.imageAnalysis.defaultModel
    });

    // Build naming context string
    let namingContextStr = '';
    if (namingContext && namingContext.length > 0) {
      const examples = namingContext
        .slice(0, 3)
        .map((n) => `"${n}"`)
        .join(', ');
      namingContextStr = `\nNaming examples from similar files: ${examples}. Reuse style only when it fits this image.`;
    }

    // Build folder categories string for the prompt (include descriptions)
    let folderCategoriesStr = '';
    if (smartFolders && smartFolders.length > 0) {
      const validFolders = smartFolders
        .filter((f) => f && typeof f.name === 'string' && f.name.trim().length > 0)
        .slice(0, TRUNCATION.FOLDERS_DISPLAY)
        .map((f) => ({
          name: f.name.trim().slice(0, TRUNCATION.NAME_MAX),
          description: (f.description || '').trim().slice(0, TRUNCATION.DESCRIPTION_MAX)
        }));
      if (validFolders.length > 0) {
        const folderListDetailed = validFolders
          .map((f, i) => `${i + 1}. "${f.name}" — ${f.description || 'no description provided'}`)
          .join('\n');

        folderCategoriesStr = `\nAvailable smart folders (name — description):\n${folderListDetailed}\nUse category EXACTLY as one folder name above. Do not invent categories.`;
      }
    }

    // Build OCR grounding context if text was extracted
    const ocrGroundingStr =
      extractedText && extractedText.length > 20
        ? `\nOCR text from image (ground truth):\n${extractedText.slice(0, OCR_GROUNDING_MAX_CHARS)}`
        : '';

    const prompt = `Analyze image "${originalFileName}" for automated file organization.
Prioritize visible content; use filename only as a weak hint.
${folderCategoriesStr}
${ocrGroundingStr}
${namingContextStr}
Return ONLY valid raw JSON (no markdown/code fences) matching this schema exactly:
${JSON.stringify(IMAGE_ANALYSIS_SCHEMA)}
Rules:
- category must be one of the available folder names when provided.
- reasoning must briefly justify the category choice.
- keywords: 3-7 terms from visual/OCR content.
- suggestedName: concise snake_case, no file extension.
- If uncertain, use null.`;

    let modelToUse = options?.visionModel;
    if (!modelToUse) {
      let cfg = {};
      try {
        cfg = await getLlamaService().getConfig();
      } catch (e) {
        logger.warn('[IMAGE] Failed to get Llama config, using defaults', { error: e.message });
      }
      modelToUse =
        cfg.visionModel ||
        AppConfig.ai.imageAnalysis.defaultModel ||
        'llava-v1.6-mistral-7b-Q4_K_M.gguf';
    }

    // Use deduplicator to prevent duplicate LLM calls for identical images
    // Include 'type' to prevent cross-file cache contamination with document analysis
    const safeFolders = Array.isArray(smartFolders) ? smartFolders : [];
    // Use a proper hash instead of slicing the first N chars of base64.
    // Slicing is a weak signature: images with similar headers/metadata can
    // share the same prefix, causing false dedup hits. MD5 is fast and
    // collision-resistant enough for cache keying (not security-sensitive).
    const imageHash = crypto.createHash('md5').update(imageBase64).digest('hex');
    const deduplicationKey = globalDeduplicator.generateKey({
      type: 'image', // Prevent cross-type contamination with document analysis
      fileName: originalFileName,
      imageHash,
      model: modelToUse,
      // FIX: Guard against null/undefined elements in safeFolders array
      folders: safeFolders
        .map((f) => f?.name || '')
        .filter(Boolean)
        .join(',')
    });

    const llamaService = getLlamaService();

    // IMPORTANT: Vision model calls can hang indefinitely if the model/server gets stuck.
    // Enforce a hard timeout and abort the underlying request (supported by the AI client).
    const timeoutMs = Number(AppConfig.ai.imageAnalysis.timeout) || 60000;
    const analysisFn = (abortController) =>
      llamaService.analyzeImage({
        prompt,
        imageBase64,
        maxTokens: AppConfig.ai.imageAnalysis.maxTokens,
        temperature: AppConfig.ai.imageAnalysis.temperature,
        signal: abortController.signal
      });

    const response = await withAbortableTimeout(
      (abortController) => {
        // Bypass deduplicator during forced reanalysis so stale cached results
        // are not served when the user explicitly requests fresh analysis.
        if (bypassDedup) {
          return analysisFn(abortController);
        }
        return globalDeduplicator.deduplicate(deduplicationKey, () => analysisFn(abortController), {
          type: 'image',
          fileName: originalFileName
        });
      },
      timeoutMs,
      `Image analysis for ${originalFileName}`
    );

    logger.info('[IMAGE-ANALYSIS] Vision request completed', {
      fileName: originalFileName,
      model: modelToUse,
      elapsedMs: Date.now() - startedAt
    });

    if (response.response) {
      try {
        const normalizedResponse = stripJsonCodeFence(response.response);
        // Use robust JSON extraction with repair for malformed LLM responses
        let parsedJson = extractAndParseJSON(normalizedResponse, null, {
          source: 'imageAnalysis',
          fileName: originalFileName,
          model: modelToUse
        });

        if (!parsedJson) {
          const repairedResponse = await attemptJsonRepairWithLlama(
            llamaService,
            normalizedResponse,
            {
              schema: IMAGE_ANALYSIS_SCHEMA,
              maxTokens: AppConfig.ai.imageAnalysis.maxTokens,
              operation: 'Image analysis'
            }
          );
          if (repairedResponse) {
            parsedJson = extractAndParseJSON(repairedResponse, null, {
              source: 'imageAnalysis.repair',
              fileName: originalFileName,
              model: modelToUse
            });
          }
        }

        if (!parsedJson || typeof parsedJson !== 'object') {
          logger.warn('[IMAGE-ANALYSIS] JSON repair failed, using fallback', {
            fileName: originalFileName,
            model: modelToUse
          });
          throw new Error('Failed to parse image analysis JSON from AI engine');
        }

        // Validate and structure the date
        if (parsedJson.date) {
          const dateObj = new Date(parsedJson.date);
          if (isNaN(dateObj.getTime())) {
            delete parsedJson.date;
            logger.warn('AI engine returned an invalid date for image, omitting.');
          } else {
            parsedJson.date = dateObj.toISOString().split('T')[0];
          }
        }

        // Ensure array fields are initialized if undefined
        let finalKeywords = Array.isArray(parsedJson.keywords) ? parsedJson.keywords : [];
        const finalColors = Array.isArray(parsedJson.colors) ? parsedJson.colors : [];

        // FALLBACK: If keywords are empty, use intelligent fallback from filename
        if (finalKeywords.length === 0) {
          finalKeywords = getIntelligentImageKeywords(
            originalFileName,
            path.extname(originalFileName)
          );
          logger.debug('[IMAGE-ANALYSIS] Fallback keywords generated', { keywords: finalKeywords });
        }

        // Ensure confidence is numeric and within expected range.
        // Some models return confidence as strings like "85" or "85%".
        const parsedConfidence = Number.parseFloat(String(parsedJson.confidence ?? ''));
        if (!Number.isFinite(parsedConfidence) || parsedConfidence < 60 || parsedConfidence > 100) {
          parsedJson.confidence = 75; // Fixed default when AI engine returns invalid confidence
          logger.debug('[IMAGE-ANALYSIS] Invalid confidence from AI engine, using default: 75');
        } else {
          parsedJson.confidence = Math.round(parsedConfidence);
        }

        return {
          ...parsedJson,
          keywords: finalKeywords,
          colors: finalColors,
          has_text: Boolean(parsedJson.has_text)
        };
      } catch (e) {
        logger.error('Error parsing AI JSON response for image', {
          error: e.message
        });
        return {
          error: 'Failed to parse image analysis from AI engine.',
          keywords: [],
          confidence: 65
        };
      }
    }

    return {
      error: 'No content in AI response for image',
      keywords: [],
      confidence: 60
    };
  } catch (error) {
    logger.error('Error calling AI engine for image', {
      error: error.message
    });

    if (
      error?.name === 'AbortError' ||
      String(error?.message || '')
        .toLowerCase()
        .includes('aborted') ||
      String(error?.message || '')
        .toLowerCase()
        .includes('timed out')
    ) {
      return {
        error:
          'Image analysis timed out. The vision model is too slow for your hardware (CPU mode). Try switching to a smaller model or increasing the timeout in settings.',
        keywords: [],
        confidence: 0
      };
    }

    // Specific handling for zero-length image error
    if (error.message.includes('zero length image')) {
      return {
        error: 'Image is empty or corrupted - cannot analyze zero-length image',
        keywords: [],
        confidence: 0
      };
    }
    // Guidance for vision model input failures
    if (error.message.includes('unable to make llava embedding')) {
      return {
        error:
          'Unsupported image format or dimensions for vision model. Convert to PNG/JPG and keep under ~2048px on the longest side.',
        keywords: [],
        confidence: 0
      };
    }

    return {
      error: `AI engine error for image: ${error.message}`,
      keywords: [],
      confidence: 60
    };
  }
}

// ============================================================================
// Hallucination Detection and Validation
// ============================================================================

/**
 * Validate analysis result against filename to detect hallucinations
 * Penalizes confidence when AI output contradicts filename context
 * @param {Object} analysis - AI analysis result
 * @param {string} fileName - Original file name
 * @param {string} extractedText - OCR-extracted text (optional)
 * @returns {Object} Analysis with validation applied and warnings added
 */
function validateAnalysisConsistency(analysis, fileName, extractedText = null) {
  if (!analysis || typeof analysis !== 'object') return analysis;

  const warnings = [];
  const fileNameLower = fileName.toLowerCase();
  const suggestedLower = (analysis.suggestedName || '').toLowerCase();
  const keywordsStr = (analysis.keywords || []).join(' ').toLowerCase();

  // Define filename context indicators
  const financialTerms = [
    'financial',
    'budget',
    'invoice',
    'receipt',
    'tax',
    'expense',
    'payment',
    'billing',
    'accounting'
  ];
  const documentTerms = ['report', 'document', 'form', 'statement', 'summary', 'analysis'];
  const landscapeTerms = [
    'sunset',
    'beach',
    'landscape',
    'nature',
    'mountain',
    'ocean',
    'forest',
    'sky',
    'scenic'
  ];

  // Check if filename indicates financial content
  const filenameIsFinancial = financialTerms.some((term) => fileNameLower.includes(term));
  const filenameIsDocument = documentTerms.some((term) => fileNameLower.includes(term));

  // Check if suggested name indicates landscape/scenic content
  const suggestedIsLandscape = landscapeTerms.some((term) => suggestedLower.includes(term));

  // CRITICAL: Detect financial document → landscape hallucination
  // FIX Bug #22: Only flag if filename does NOT contain landscape terms (avoid false positives like "financial-district-sunset.jpg")
  const filenameHasLandscapeContext = landscapeTerms.some((term) => fileNameLower.includes(term));

  if (filenameIsFinancial && suggestedIsLandscape && !filenameHasLandscapeContext) {
    warnings.push('HALLUCINATION DETECTED: Suggested landscape name for financial document');
    analysis.confidence = Math.min(analysis.confidence || 75, 25);
    analysis.hallucination_detected = true;

    // Override with filename-based suggestion
    analysis.suggestedName = safeSuggestedName(fileName, '');
    logger.warn('[HALLUCINATION] Landscape suggested for financial file, overriding', {
      original: suggestedLower,
      corrected: analysis.suggestedName
    });
  }

  // Check if document filename got landscape analysis
  if (filenameIsDocument && suggestedIsLandscape) {
    warnings.push('HALLUCINATION DETECTED: Suggested landscape name for document file');
    analysis.confidence = Math.min(analysis.confidence || 75, 30);
    analysis.hallucination_detected = true;
    analysis.suggestedName = safeSuggestedName(fileName, '');
  }

  // Check if keywords contradict filename
  if (
    filenameIsFinancial &&
    !keywordsStr.includes('financ') &&
    !keywordsStr.includes('budget') &&
    !keywordsStr.includes('invoice') &&
    !keywordsStr.includes('money') &&
    !keywordsStr.includes('business')
  ) {
    warnings.push('Keywords do not reflect financial context from filename');
    analysis.confidence = Math.max(40, (analysis.confidence || 75) - 20);

    // Inject financial keywords from filename
    const fileKeywords = financialTerms.filter((term) => fileNameLower.includes(term));
    if (fileKeywords.length > 0 && Array.isArray(analysis.keywords)) {
      analysis.keywords = [...fileKeywords, ...analysis.keywords].slice(0, 7);
    }
  }

  // Validate against OCR-extracted text
  if (extractedText && extractedText.length > 30) {
    const textLower = extractedText.toLowerCase();
    const textIsFinancial =
      financialTerms.some((term) => textLower.includes(term)) ||
      textLower.includes('$') ||
      textLower.includes('total') ||
      textLower.includes('amount');

    if (textIsFinancial && suggestedIsLandscape) {
      warnings.push('OCR text contains financial content but analysis suggests landscape');
      analysis.confidence = Math.min(analysis.confidence || 75, 20);
      analysis.hallucination_detected = true;
      analysis.suggestedName = safeSuggestedName(fileName, '');
    }
  }

  // Check content_type consistency
  if (
    (filenameIsFinancial || filenameIsDocument) &&
    analysis.content_type &&
    ['landscape', 'nature', 'scenic', 'beach', 'sunset'].includes(
      analysis.content_type.toLowerCase()
    )
  ) {
    warnings.push('Content type contradicts document/financial filename');
    analysis.content_type = 'text_document';
    analysis.confidence = Math.max(35, (analysis.confidence || 75) - 25);
  }

  // CROSS-VALIDATION: Use fallbackUtils intelligent category as ground truth
  // If AI category doesn't match filename-based category, prefer filename-based
  const fileExtension = fileName.includes('.') ? fileName.slice(fileName.lastIndexOf('.')) : '';
  const intelligentCategory = getIntelligentImageCategory(fileName, fileExtension);

  if (analysis.category && intelligentCategory) {
    const analysisCategoryLower = analysis.category.toLowerCase();
    const intelligentCategoryLower = intelligentCategory.toLowerCase();

    // Check if AI category is generic but intelligent category is specific
    const genericCategories = ['documents', 'images', 'files', 'work', 'general', 'other', 'misc'];
    const analysisIsGeneric = genericCategories.some((g) => analysisCategoryLower.includes(g));
    const intelligentIsSpecific = !genericCategories.some((g) =>
      intelligentCategoryLower.includes(g)
    );

    if (analysisIsGeneric && intelligentIsSpecific) {
      warnings.push(
        `Category override: "${analysis.category}" → "${intelligentCategory}" (filename-based)`
      );
      analysis.original_category = analysis.category;
      analysis.category = intelligentCategory;
      analysis.category_source = 'filename_fallback';
    }

    // If AI gave a completely wrong category for financial files
    if (
      filenameIsFinancial &&
      !analysisCategoryLower.includes('financ') &&
      !analysisCategoryLower.includes('budget')
    ) {
      if (intelligentCategoryLower.includes('financ') || intelligentCategoryLower === 'financial') {
        warnings.push(
          `Financial category override: "${analysis.category}" → "${intelligentCategory}"`
        );
        analysis.original_category = analysis.category;
        analysis.category = intelligentCategory;
        analysis.category_source = 'filename_financial_override';
      }
    }
  }

  // Log warnings if any detected
  if (warnings.length > 0) {
    analysis.validation_warnings = warnings;
    logger.warn('[IMAGE-VALIDATION] Analysis inconsistencies detected', {
      fileName,
      warnings,
      adjustedConfidence: analysis.confidence,
      hallucinationDetected: analysis.hallucination_detected || false,
      categorySource: analysis.category_source || 'llama'
    });
  }

  return analysis;
}

// ============================================================================
// Helper Functions - Extracted for readability and maintainability
// ============================================================================

// createFallbackResult and normalizeCategoryToSmartFolder removed
// Now using createFallbackAnalysis from ./fallbackUtils
// For category normalization, use FolderMatchingService.matchCategoryToFolder directly

/**
 * Extract EXIF date from image metadata
 * @param {Buffer} imageBuffer - Image buffer
 * @returns {Promise<string|null>} EXIF date in YYYY-MM-DD format or null
 */
async function extractExifDate(imageBuffer) {
  try {
    const meta = await sharp(imageBuffer).metadata();
    if (meta && meta.exif) {
      const exif = require('exif-reader')(meta.exif);
      if (exif && (exif.exif || exif.image)) {
        const dateVal = exif.exif?.DateTimeOriginal || exif.image?.ModifyDate;
        if (dateVal) {
          let dateResult;
          if (dateVal instanceof Date) {
            // exif-reader v2 returns Date objects
            if (!isNaN(dateVal.getTime())) {
              dateResult = dateVal.toISOString().split('T')[0];
            }
          } else {
            // Legacy string format: "2024:01:15 10:30:00"
            const parts = String(dateVal).split(' ')[0].replace(/:/g, '-');
            if (parts.match(/^\d{4}-\d{2}-\d{2}$/)) {
              dateResult = parts;
            }
          }
          if (dateResult) {
            logger.debug(`[IMAGE] Extracted EXIF date: ${dateResult}`);
            return dateResult;
          }
        }
      }
    }
  } catch (exifErr) {
    // Non-fatal if metadata extraction fails
    logger.debug('[IMAGE] EXIF extraction failed:', exifErr.message);
  }
  return null;
}

/**
 * Preprocess image for vision model compatibility
 * Converts unsupported formats to PNG and resizes large images
 * @param {Buffer} imageBuffer - Original image buffer
 * @param {string} fileExtension - File extension
 * @returns {Promise<Buffer>} Processed image buffer
 */
async function preprocessImageBuffer(imageBuffer, fileExtension) {
  // Wrap entire preprocessing in try/catch to catch synchronous sharp errors
  try {
    const needsFormatConversion = [
      '.svg',
      '.tiff',
      '.tif',
      '.bmp',
      '.gif',
      '.webp',
      '.heic',
      '.ico'
    ].includes(fileExtension);
    const maxDimension = 1024; // Reduced from 1536 to prevent timeouts on large images while maintaining quality

    const lenientFormats = ['.tiff', '.tif', '.bmp'];
    const sharpOptions = lenientFormats.includes(fileExtension) ? { failOn: 'none' } : undefined;

    let meta = null;
    try {
      meta = await sharp(imageBuffer, sharpOptions).metadata();
    } catch (metaErr) {
      logger.debug('[IMAGE] Metadata extraction failed:', metaErr.message);
    }

    const shouldResize =
      meta && (Number(meta.width) > maxDimension || Number(meta.height) > maxDimension);

    if (!needsFormatConversion && !shouldResize) {
      return imageBuffer;
    }

    let transformer = sharp(imageBuffer, sharpOptions);
    if (shouldResize) {
      const resizeOptions = { fit: 'inside', withoutEnlargement: true };
      if (meta && meta.width && meta.height) {
        if (meta.width >= meta.height) resizeOptions.width = maxDimension;
        else resizeOptions.height = maxDimension;
      } else {
        resizeOptions.width = maxDimension;
      }
      transformer = transformer.resize(resizeOptions);
    }
    return await transformer.png({ compressionLevel: 5 }).toBuffer();
  } catch (error) {
    logger.error('[IMAGE] Preprocessing failed:', error.message);
    throw error;
  }
}

// ============================================================================
// Main Analysis Function
// ============================================================================

async function analyzeImageFile(filePath, smartFolders = [], options = {}) {
  logger.info(`Analyzing image file`, { path: filePath });
  const bypassCache = Boolean(options?.bypassCache);
  const fileExtension = path.extname(filePath).toLowerCase();
  const fileName = path.basename(filePath);
  const fileNameLower = fileName.toLowerCase();
  const resolvedSmartFolder = findContainingSmartFolder(filePath, smartFolders);
  const isInSmartFolder = Boolean(resolvedSmartFolder);
  const hasTextHint = hasTextNameHint(fileNameLower);
  let ocrAttempted = false;
  const smartFolderSig = Array.isArray(smartFolders)
    ? smartFolders
        .map((f) => {
          const name = typeof f?.name === 'string' ? f.name.trim() : '';
          const description = typeof f?.description === 'string' ? f.description.trim() : '';
          if (!name && !description) return '';
          return `${name}:${description}`;
        })
        .filter(Boolean)
        .sort()
        .join('|')
    : '';

  // Check if file extension is supported (include SVG by rasterizing via sharp)
  const supportedExtensions = SUPPORTED_IMAGE_EXTENSIONS;
  if (!supportedExtensions.includes(fileExtension)) {
    return {
      error: `Unsupported image format: ${fileExtension}`,
      category: 'unsupported',
      keywords: [],
      confidence: 0,
      suggestedName:
        path.basename(fileName, fileExtension).replace(/[^a-zA-Z0-9_-]/g, '_') + fileExtension
    };
  }

  try {
    const llamaService = getLlamaService();
    let preflight = await getImagePreflight(llamaService, {
      forceRefresh: bypassCache
    });
    if (!preflight.isHealthy) {
      logger.warn('[IMAGE] Preflight unhealthy, retrying once with forced refresh', {
        status: preflight.health?.status
      });
      preflight = await getImagePreflight(llamaService, {
        forceRefresh: true
      });
    }
    if (!preflight.isHealthy) {
      logger.warn('[ANALYSIS-FALLBACK] AI engine unavailable, using filename-based analysis', {
        status: preflight.health?.status
      });
      return createFallbackAnalysis({
        fileName,
        fileExtension,
        reason: 'AI engine unavailable',
        smartFolders,
        confidence: 60,
        type: 'image'
      });
    }
    // Resolve vision model for cache signatures
    const visionModelName = preflight.visionModelName;
    const visionModelListed = preflight.modelNames.includes(visionModelName);
    if (!visionModelListed) {
      logger.warn('[IMAGE] Vision model not listed; will attempt vision analysis anyway', {
        requiredModel: visionModelName,
        availableModels: preflight.modelNames.slice(0, 5)
      });
    }

    // First, check if file exists and has content
    let stats;
    try {
      stats = await fs.stat(filePath);
    } catch (statError) {
      // Handle ENOENT and other file access errors gracefully
      if (statError.code === 'ENOENT') {
        logger.error(`Image file does not exist`, { path: filePath });
        return {
          error: 'Image file not found (ENOENT)',
          category: 'error',
          keywords: [],
          confidence: 0
        };
      }
      throw statError; // Re-throw other errors
    }

    if (stats.size === 0) {
      logger.error(`Image file is empty`, { path: filePath });
      return {
        error: 'Image file is empty (0 bytes)',
        category: 'error',
        keywords: [],
        confidence: 0
      };
    }

    logger.debug(`Image file size`, { bytes: stats.size });

    // Guard against oversized images that would exhaust memory
    const MAX_IMAGE_SIZE = 100 * 1024 * 1024; // 100MB
    if (stats.size > MAX_IMAGE_SIZE) {
      logger.error(`Image file too large`, {
        bytes: stats.size,
        maxBytes: MAX_IMAGE_SIZE,
        path: filePath
      });
      return {
        error: `Image file too large (${(stats.size / 1024 / 1024).toFixed(1)}MB exceeds ${MAX_IMAGE_SIZE / 1024 / 1024}MB limit)`,
        category: 'error',
        keywords: [],
        confidence: 0
      };
    }

    // Handle TOCTOU race condition - file could be deleted between stat() and readFile()
    let imageBuffer;
    try {
      imageBuffer = await fs.readFile(filePath);
    } catch (readError) {
      if (readError.code === 'ENOENT') {
        logger.error(`Image file disappeared during read`, { path: filePath });
        return {
          error: 'Image file was deleted during analysis (TOCTOU)',
          category: 'error',
          keywords: [],
          confidence: 0
        };
      }
      throw readError; // Re-throw other errors
    }

    // Cache quick path: signature based on file stats
    // LRUCache handles TTL automatically - get() returns null for expired entries
    const signature = `${IMAGE_SIGNATURE_VERSION}|${visionModelName}|${smartFolderSig}|${filePath}|${stats.size}|${stats.mtimeMs}`;
    if (!bypassCache) {
      const cached = getImageAnalysisCache().get(signature);
      if (cached != null) {
        return cached;
      }
    } else {
      logger.debug('[IMAGE] Bypassing analysis cache for reanalysis', { filePath });
    }

    // Extract EXIF date and preprocess image using helpers
    const exifDate = await extractExifDate(imageBuffer);
    try {
      imageBuffer = await preprocessImageBuffer(imageBuffer, fileExtension);
    } catch (preErr) {
      logger.error(`Failed to pre-process image for analysis`, {
        path: filePath,
        error: preErr.message
      });
      const preprocessingFallback = createFallbackAnalysis({
        fileName,
        fileExtension,
        reason: 'image preprocessing failed',
        smartFolders,
        confidence: 40,
        type: 'image'
      });
      preprocessingFallback.analysisWarning = preErr.message;
      return preprocessingFallback;
    }

    // Validate buffer is not empty
    if (imageBuffer.length === 0) {
      logger.error(`Image buffer is empty after reading`, { path: filePath });
      return {
        error: 'Image buffer is empty after reading',
        category: 'error',
        keywords: [],
        confidence: 0
      };
    }

    logger.debug(`Image buffer size`, { bytes: imageBuffer.length });
    const ocrSourceBuffer = imageBuffer;
    const imageBase64 = imageBuffer.toString('base64');
    // Release image buffer immediately after base64 conversion
    // This prevents holding potentially large (10MB+) buffers in memory during
    // subsequent async operations (AI analysis, semantic matching, etc.)
    imageBuffer = null;

    // Validate base64 encoding
    if (!imageBase64 || imageBase64.length === 0) {
      logger.error(`Image base64 encoding failed`, { path: filePath });
      return {
        error: 'Image base64 encoding failed',
        category: 'error',
        keywords: [],
        confidence: 0
      };
    }

    logger.debug(`Base64 length`, { chars: imageBase64.length });

    // ANTI-HALLUCINATION: Extract text from image FIRST to ground the vision analysis
    // This provides OCR-based ground truth that can validate/override visual interpretation
    let extractedText = null;
    try {
      // Only attempt OCR for images that might contain text (documents, screenshots, etc.)
      if (hasTextHint) {
        logger.debug('[IMAGE] Filename suggests document content, attempting OCR pre-extraction');
        ocrAttempted = true;
        extractedText = await extractTextFromImage(filePath, {
          allowVisionOcr: true,
          buffer: ocrSourceBuffer,
          fileExtension,
          preprocessed: true
        });
        if (extractedText && extractedText.length > 20) {
          logger.info('[IMAGE] OCR pre-extraction successful', {
            textLength: extractedText.length,
            preview: extractedText.slice(0, 100)
          });
        }
      }
    } catch (ocrError) {
      // Non-fatal: OCR failure doesn't block analysis, just means no grounding
      logger.debug('[IMAGE] OCR pre-extraction failed (non-fatal):', ocrError.message);
    }

    // Analyze with AI engine, passing extracted text for grounding
    let namingContext = [];
    try {
      // Try to get folder matcher to find similar files for naming context
      const { matcher } = getServices();
      if (matcher) {
        // Only if we have some text or at least a filename to embed
        const textForEmbedding = (extractedText || fileName || '').slice(0, 1000);
        if (textForEmbedding) {
          if (!matcher.embeddingCache?.initialized) {
            await matcher.initialize();
          }
          const { vector } = await matcher.embedText(textForEmbedding);
          const similarFiles = await matcher.findSimilarFilesByVector(vector, 5);
          namingContext = similarFiles
            .filter((f) => f.metadata && f.metadata.name && f.metadata.name !== fileName)
            .map((f) => f.metadata.name);

          if (namingContext.length > 0) {
            logger.debug('[IMAGE] Found similar files for naming context', {
              count: namingContext.length
            });
          }
        }
      }
    } catch (ncError) {
      logger.debug('[IMAGE] Failed to get naming context', { error: ncError.message });
    }

    const fileDateForText = exifDate
      ? exifDate
      : new Date(stats.mtimeMs || Date.now()).toISOString().split('T')[0];
    const attemptTextFallback = async () => {
      let ocrText = extractedText;
      if (!ocrText || ocrText.length < 20) {
        logger.debug('[IMAGE] Attempting OCR-only fallback for missing vision model');
        ocrAttempted = true;
        ocrText = await extractTextFromImage(filePath, {
          allowVisionOcr: false,
          preferTesseract: true,
          buffer: ocrSourceBuffer,
          fileExtension,
          preprocessed: true
        });
      }
      if (!ocrText || ocrText.length < 20) {
        return null;
      }
      extractedText = ocrText;
      const textAnalysis = await analyzeTextWithLlama(
        ocrText,
        fileName,
        smartFolders,
        fileDateForText,
        namingContext,
        { bypassCache }
      );
      if (textAnalysis && !textAnalysis.error) {
        return {
          ...textAnalysis,
          has_text: true,
          content_type: 'text_document',
          colors: Array.isArray(textAnalysis.colors) ? textAnalysis.colors : []
        };
      }
      return textAnalysis;
    };

    let analysis;
    try {
      let visionSupported = true;
      try {
        visionSupported = await llamaService.supportsVisionInput();
      } catch {
        visionSupported = false;
      }

      if (!visionSupported) {
        logger.warn('[IMAGE] Vision input unsupported, falling back to OCR/text', {
          filePath
        });
        const fallbackAnalysis = await attemptTextFallback();
        if (fallbackAnalysis) {
          analysis = fallbackAnalysis;
        } else {
          analysis = createVisionUnavailableFallback(
            {
              fileName,
              fileExtension,
              reason: `Vision model '${visionModelName}' not loaded`,
              smartFolders,
              confidence: 55,
              type: 'image'
            },
            extractedText
          );
          analysis._isFallback = true;
        }
      } else {
        analysis = await analyzeImageWithLlama(
          imageBase64,
          fileName,
          smartFolders,
          extractedText,
          namingContext,
          { bypassCache, visionModel: visionModelName }
        );
      }
    } catch (error) {
      if (isModelNotAvailableError(error)) {
        logger.warn('[IMAGE] Vision analysis unavailable, falling back to OCR/text', {
          error: error.message,
          filePath
        });
        const fallbackAnalysis = await attemptTextFallback();
        if (fallbackAnalysis) {
          analysis = fallbackAnalysis;
        } else {
          analysis = createVisionUnavailableFallback(
            {
              fileName,
              fileExtension,
              reason: `Vision model '${visionModelName}' not loaded`,
              smartFolders,
              confidence: 55,
              type: 'image'
            },
            extractedText
          );
          analysis._isFallback = true;
        }
      } else {
        const recoverable = isRecoverableVisionResultError(error);
        logger.error('[IMAGE] Error calling analyzeImageWithLlama', {
          error: error.message,
          filePath,
          recoverable
        });
        if (recoverable) {
          const fallbackAnalysis = await attemptTextFallback();
          if (fallbackAnalysis) {
            fallbackAnalysis.analysisWarning = error.message;
            analysis = fallbackAnalysis;
          }
        }
        if (!analysis) {
          analysis = createVisionUnavailableFallback(
            {
              fileName,
              fileExtension,
              reason: 'analysis error',
              smartFolders,
              confidence: 55,
              type: 'image'
            },
            extractedText
          );
          analysis.analysisWarning = error.message;
          analysis._isFallback = true;
        }
      }
    }

    if (
      analysis?.error &&
      (isModelNotAvailableError(analysis.error) || isRecoverableVisionResultError(analysis.error))
    ) {
      logger.warn('[IMAGE] Vision analysis reported recoverable error, falling back to OCR/text', {
        error: analysis.error
      });
      const fallbackAnalysis = await attemptTextFallback();
      if (fallbackAnalysis) {
        fallbackAnalysis.analysisWarning = analysis.error;
        analysis = fallbackAnalysis;
      } else {
        analysis = createVisionUnavailableFallback(
          {
            fileName,
            fileExtension,
            reason: `Vision model '${visionModelName}' not loaded`,
            smartFolders,
            confidence: 55,
            type: 'image'
          },
          extractedText
        );
        analysis._isFallback = true;
      }
    }

    if (analysis?.error && !isModelNotAvailableError(analysis.error)) {
      const initialVisionError = String(analysis.error);
      logger.warn('[IMAGE] Vision analysis returned error, retrying once with fresh request', {
        error: initialVisionError,
        filePath
      });
      try {
        const retriedAnalysis = await analyzeImageWithLlama(
          imageBase64,
          fileName,
          smartFolders,
          extractedText,
          namingContext,
          { bypassCache: true, visionModel: visionModelName }
        );
        if (retriedAnalysis && !retriedAnalysis.error) {
          analysis = retriedAnalysis;
          logger.info('[IMAGE] Vision retry succeeded', { filePath });
        } else if (retriedAnalysis?.error) {
          analysis = retriedAnalysis;
        }
      } catch (retryError) {
        logger.warn('[IMAGE] Vision retry failed', {
          filePath,
          error: retryError?.message || String(retryError)
        });
      }
    }

    if (analysis?.error) {
      logger.warn('[IMAGE] Attempting final OCR/text fallback after vision error', {
        error: analysis.error,
        filePath
      });
      const finalTextFallback = await attemptTextFallback();
      if (finalTextFallback && !finalTextFallback.error) {
        finalTextFallback.analysisWarning = analysis.error;
        analysis = finalTextFallback;
        logger.info('[IMAGE] Final OCR/text fallback succeeded', { filePath });
      }
    }

    // Merge EXIF date if available and AI didn't return a valid date (or override it?)
    // The plan says "populate the date field without asking LLM".
    // We'll trust EXIF over LLM hallucination if EXIF is present.
    if (exifDate) {
      if (!analysis) analysis = {};
      analysis.date = exifDate;
    }

    // Skip validation and OCR for fallback analyses — they have no AI output to validate.
    // Fall through directly to embedding so fallback images become searchable.
    if (!analysis?._isFallback) {
      // ANTI-HALLUCINATION: Validate analysis against filename and OCR text
      if (analysis && !analysis.error) {
        analysis = validateAnalysisConsistency(analysis, fileName, extractedText);
      }

      // If OCR not performed earlier, attempt it now when the analysis indicates text content.
      if (
        (!extractedText || extractedText.length < 20) &&
        analysis &&
        !analysis.error &&
        !ocrAttempted
      ) {
        const wantsOcr = shouldRunPostVisionOcr(analysis, hasTextHint);
        if (wantsOcr) {
          try {
            logger.debug('[IMAGE] Analysis indicates text content, attempting OCR post-extraction');
            ocrAttempted = true;
            extractedText = await extractTextFromImage(filePath, {
              allowVisionOcr: true,
              buffer: ocrSourceBuffer,
              fileExtension,
              preprocessed: true
            });
            if (extractedText && extractedText.length > 20) {
              logger.info('[IMAGE] OCR post-extraction successful', {
                textLength: extractedText.length,
                preview: extractedText.slice(0, 100)
              });
              analysis = validateAnalysisConsistency(analysis, fileName, extractedText);
            }
          } catch (ocrError) {
            logger.debug('[IMAGE] OCR post-extraction failed (non-fatal):', ocrError.message);
          }
        } else {
          logger.debug('[IMAGE] Skipping OCR post-extraction (analysis already sufficient)', {
            confidence: analysis?.confidence,
            hasTextHint
          });
        }
      }
    }

    const extractedTextForStorage = normalizeExtractedTextForStorage(extractedText);

    // Semantic folder refinement using embeddings
    // MIGRATION: Now uses unified semanticFolderMatcher module
    try {
      await applyUnifiedFolderMatching({
        analysis,
        filePath,
        fileName: path.basename(filePath),
        fileExtension: path.extname(filePath),
        fileSize: stats?.size,
        smartFolders,
        extractedText: extractedTextForStorage,
        type: 'image'
      });

      // Explicitly queue embedding for images to ensure they are searchable
      // even if they don't have OCR text (using keywords/description instead)
      const { matcher } = getServices();
      const gate = await shouldEmbed({ stage: 'analysis', isInSmartFolder });
      if (matcher && analysis && gate.shouldEmbed) {
        const persistedEmbedding = analysis?._embeddingForPersistence;
        let usedPrecomputedEmbedding = false;
        if (
          persistedEmbedding &&
          Array.isArray(persistedEmbedding.vector) &&
          persistedEmbedding.vector.length > 0
        ) {
          const queueCapacity =
            typeof embeddingQueueManager.waitForAnalysisQueueCapacity === 'function'
              ? await embeddingQueueManager.waitForAnalysisQueueCapacity({
                  highWatermarkPercent: 75,
                  releasePercent: 50,
                  maxWaitMs: 60000
                })
              : { timedOut: false, capacityPercent: null };
          if (queueCapacity.timedOut) {
            logger.warn('[IMAGE] Analysis embedding queue remained saturated before enqueue', {
              path: filePath,
              capacityPercent: queueCapacity.capacityPercent
            });
          }
          await embeddingQueueManager.removeByFilePath?.(filePath);
          await analysisQueue.enqueue({
            id: getSemanticFileId(filePath),
            path: filePath,
            text:
              persistedEmbedding.meta?.summary || analysis.summary || analysis.suggestedName || '',
            vector: persistedEmbedding.vector,
            model: persistedEmbedding.model,
            meta: {
              ...persistedEmbedding.meta,
              path: filePath,
              filePath,
              fileName,
              name: fileName,
              fileSize: stats?.size,
              fileExtension,
              fileType: 'image',
              analysis,
              type: 'image',
              smartFolder: resolvedSmartFolder?.name || null,
              smartFolderPath: resolvedSmartFolder?.path || null
            }
          });
          logger.debug('[IMAGE] Queued precomputed embedding for persistence', { path: filePath });
          usedPrecomputedEmbedding = true;
        }

        if (!usedPrecomputedEmbedding) {
          // Embedding scope is controlled by the embeddingScope setting:
          // - 'all_analyzed' (default): embed every analyzed image
          // - 'smart_folders_only': only embed images in a configured smart folder
          const textParts = [
            analysis.suggestedName,
            analysis.summary,
            analysis.category,
            ...(analysis.keywords || [])
          ].filter(Boolean);

          // Include OCR text if available to boost search relevance
          if (extractedTextForStorage) {
            textParts.push(extractedTextForStorage.slice(0, 1000));
          }

          const textToEmbed = textParts.join(' ');
          if (textToEmbed.length > 0) {
            // Initialize if needed
            if (!matcher.embeddingCache?.initialized) {
              await matcher.initialize();
            }

            const { vector } = await matcher.embedText(textToEmbed);
            if (vector) {
              const queueCapacity =
                typeof embeddingQueueManager.waitForAnalysisQueueCapacity === 'function'
                  ? await embeddingQueueManager.waitForAnalysisQueueCapacity({
                      highWatermarkPercent: 75,
                      releasePercent: 50,
                      maxWaitMs: 60000
                    })
                  : { timedOut: false, capacityPercent: null };
              if (queueCapacity.timedOut) {
                logger.warn('[IMAGE] Analysis embedding queue remained saturated before enqueue', {
                  path: filePath,
                  capacityPercent: queueCapacity.capacityPercent
                });
              }
              await embeddingQueueManager.removeByFilePath?.(filePath);
              await analysisQueue.enqueue({
                id: getSemanticFileId(filePath),
                path: filePath,
                text: textToEmbed,
                vector,
                meta: {
                  path: filePath,
                  filePath,
                  fileName,
                  name: fileName,
                  fileSize: stats?.size,
                  fileExtension,
                  fileType: 'image',
                  analysis,
                  type: 'image',
                  smartFolder: resolvedSmartFolder?.name || null,
                  smartFolderPath: resolvedSmartFolder?.path || null
                }
              });
              logger.debug('[IMAGE] Queued embedding for persistence', { path: filePath });
            }
          }
        }
      } else if (matcher && analysis && !gate.shouldEmbed) {
        logger.debug('[IMAGE] Skipping embedding persistence by policy/timing gate', {
          path: filePath,
          timing: gate.timing,
          policy: gate.policy
        });
      }
    } catch (error) {
      logger.warn('[IMAGE] Unexpected error in semantic folder refinement or embedding:', {
        error: error?.message,
        filePath
      });
    }

    // Ensure analysis is defined before checking properties
    if (!analysis) {
      logger.warn('[IMAGE] analyzeImageWithLlama returned undefined', {
        filePath
      });
      const recoveredAnalysis = await attemptTextFallback();
      if (recoveredAnalysis) {
        recoveredAnalysis.analysisWarning = 'Vision analysis returned undefined';
        analysis = recoveredAnalysis;
      }
    }
    if (!analysis) {
      const result = createFallbackAnalysis({
        fileName,
        fileExtension,
        reason: 'undefined result',
        smartFolders,
        confidence: 60,
        type: 'image',
        options: { error: 'AI image analysis returned undefined' }
      });
      if (extractedTextForStorage) {
        result.extractedText = extractedTextForStorage;
      }
      if (!bypassCache) {
        try {
          setImageCache(signature, result);
        } catch (cacheError) {
          logger.debug('[IMAGE] Cache write failed (non-fatal):', cacheError?.message);
        }
      }
      return result;
    }

    if (analysis && !analysis.error) {
      // Ensure the original file extension is preserved in suggestedName
      let finalSuggestedName = analysis.suggestedName;
      if (finalSuggestedName && fileExtension) {
        const suggestedExt = path.extname(finalSuggestedName);
        if (!suggestedExt) {
          finalSuggestedName += fileExtension;
        }
      }
      const normalizedCategory =
        typeof analysis.category === 'string' && analysis.category.trim().length > 0
          ? FolderMatchingService.matchCategoryToFolder(analysis.category, smartFolders)
          : analysis.category;
      const normalized = normalizeAnalysisResult(
        {
          ...analysis,
          category: normalizedCategory || analysis.category,
          content_type: analysis.content_type || 'unknown',
          suggestedName: finalSuggestedName || safeSuggestedName(fileName, fileExtension),
          extractedText: extractedTextForStorage
        },
        { category: 'image', keywords: [] }
      );
      logger.info('[AI-ANALYSIS-SUCCESS] Image analyzed', {
        fileName,
        category: normalized.category,
        suggestedName: normalized.suggestedName,
        confidence: normalized.confidence,
        purpose: normalized.purpose,
        project: normalized.project,
        entity: normalized.entity,
        contentType: normalized.content_type,
        date: normalized.date,
        keywords: normalized.keywords,
        ocrText: extractedTextForStorage ? `${extractedTextForStorage.length} chars` : 'none',
        extractionMethod: 'vision'
      });

      if (!bypassCache) {
        try {
          setImageCache(signature, normalized);
        } catch (cacheError) {
          logger.warn('[IMAGE] Cache write failed:', {
            error: cacheError?.message,
            filePath
          });
        }
      }
      return normalized;
    }

    // Fallback analysis if AI engine fails
    const result = createFallbackAnalysis({
      fileName,
      fileExtension,
      reason: analysis?.error ? 'vision analysis unavailable' : 'AI engine failed',
      smartFolders,
      confidence: 60,
      type: 'image',
      options: analysis?.error ? { error: String(analysis.error) } : {}
    });
    result.isFallback = true;
    if (analysis?.error) {
      result.analysisWarning = analysis.error;
    }
    // Add null check before accessing analysis.keywords
    // Preserve keywords from partial analysis if available
    if (analysis && Array.isArray(analysis.keywords)) {
      result.keywords = analysis.keywords;
    }
    if (extractedTextForStorage) {
      result.extractedText = extractedTextForStorage;
    }
    if (!bypassCache) {
      try {
        setImageCache(signature, result);
      } catch (cacheError) {
        logger.warn('[IMAGE] Cache write failed:', {
          error: cacheError?.message,
          filePath
        });
      }
    }
    return result;
  } catch (error) {
    // Re-throw programming errors so they surface in logs/tests
    if (
      error instanceof TypeError ||
      error instanceof ReferenceError ||
      error instanceof SyntaxError ||
      error instanceof RangeError
    ) {
      throw error;
    }
    logger.error('Error processing image', { filePath, error: error.message });
    const fallback = createFallbackAnalysis({
      fileName,
      fileExtension,
      reason: error.message || 'image processing failed',
      smartFolders,
      confidence: 55,
      type: 'image'
    });
    fallback.analysisWarning = error.message;
    fallback.isFallback = true;
    return fallback;
  }
}

// OCR capability using the AI engine for text extraction from images
async function extractTextFromImage(filePath, options = {}) {
  let timeoutMs = OCR_DEFAULTS.timeoutMs;
  try {
    const MAX_OCR_SIZE = 20 * 1024 * 1024; // 20MB limit for OCR
    const fileExtension = (options.fileExtension || path.extname(filePath)).toLowerCase();
    let imageBuffer = options.buffer;
    const allowVisionOcr = options.allowVisionOcr !== false;
    const preferTesseract = options.preferTesseract !== false;
    const preprocessed = options.preprocessed === true;

    // Check file size before reading to prevent memory exhaustion
    if (imageBuffer) {
      if (imageBuffer.length > MAX_OCR_SIZE) {
        logger.warn('[IMAGE] Skipping OCR for large buffer:', {
          filePath,
          size: imageBuffer.length,
          limit: MAX_OCR_SIZE
        });
        return null;
      }
    } else {
      const stats = await fs.stat(filePath);
      if (stats.size > MAX_OCR_SIZE) {
        logger.warn('[IMAGE] Skipping OCR for large file:', {
          filePath,
          size: stats.size,
          limit: MAX_OCR_SIZE
        });
        return null;
      }
      imageBuffer = await fs.readFile(filePath);
    }

    // Preprocess image for model compatibility (resize/convert) unless already preprocessed.
    if (!preprocessed) {
      try {
        imageBuffer = await preprocessImageBuffer(imageBuffer, fileExtension);
      } catch (preprocessError) {
        logger.warn('[IMAGE-OCR] Preprocessing failed, skipping OCR', {
          filePath,
          error: preprocessError.message
        });
        return null;
      }
    }

    // Prefer Tesseract OCR for text extraction when available
    if (preferTesseract) {
      try {
        const tesseractResult = await recognizeIfAvailable(null, imageBuffer, {
          lang: 'eng',
          oem: 1,
          psm: 3
        });
        if (tesseractResult.success && tesseractResult.text && tesseractResult.text.trim()) {
          return tesseractResult.text.trim();
        }
      } catch (tessError) {
        logger.debug('[IMAGE-OCR] Tesseract OCR failed, falling back to vision OCR', {
          error: tessError.message
        });
      }
    }

    if (!allowVisionOcr) {
      return null;
    }

    const imageBase64 = imageBuffer.toString('base64');
    imageBuffer = null;

    const prompt = `Extract all readable text from this image. Return only the text content, maintaining the original structure and formatting as much as possible. If no text is found, return "NO_TEXT_FOUND".`;

    const llamaService2 = getLlamaService();
    if (!(await llamaService2.supportsVisionInput())) {
      logger.warn('[IMAGE-OCR] Vision input not supported, skipping AI OCR');
      return null;
    }
    const cfg2 = await llamaService2.getConfig();
    const modelToUse2 = cfg2.visionModel || AppConfig.ai.imageAnalysis.defaultModel;

    // FIX Bug #27: Validate OCR timeout environment variable
    const envOcrTimeout = Number(process.env.AI_OCR_TIMEOUT);
    timeoutMs =
      (envOcrTimeout > 0 ? envOcrTimeout : 0) ||
      OCR_DEFAULTS.timeoutMs ||
      TIMEOUTS.AI_ANALYSIS_MEDIUM ||
      60000;

    logger.debug('[IMAGE-OCR] Using vision model for OCR', {
      model: modelToUse2,
      fileName: path.basename(filePath),
      timeoutMs
    });
    const response = await withAbortableTimeout(
      (abortController) =>
        llamaService2.analyzeImage({
          prompt,
          imageBase64,
          maxTokens: OCR_DEFAULTS.maxTokens,
          temperature: 0.1,
          signal: abortController.signal
        }),
      timeoutMs,
      `Image OCR for ${path.basename(filePath)}`
    );

    if (response.response && response.response.trim() !== 'NO_TEXT_FOUND') {
      return response.response.trim();
    }

    return null;
  } catch (error) {
    const message = error?.message || 'OCR failed';
    const lowerMessage = message.toLowerCase();
    const isTimeout =
      error?.name === 'AbortError' ||
      lowerMessage.includes('timed out') ||
      lowerMessage.includes('aborted');
    if (isTimeout) {
      logger.warn('[IMAGE-OCR] OCR timed out, skipping', {
        fileName: path.basename(filePath),
        timeoutMs
      });
      return null;
    }
    logger.error('Error extracting text from image', { error: message });
    return null;
  }
}

// Fallback image helpers sourced from fallbackUtils

/**
 * Force flush the embedding queue (useful for cleanup or end of batch)
 */
async function flushAllEmbeddings() {
  await analysisQueue.flush();
}

/**
 * Reset module singletons and caches
 * Useful for hot reload, testing, or reconnecting to services
 */
function resetSingletons() {
  // Clear local image analysis cache (matcher singletons managed by DI container)
  getImageAnalysisCache().clear();
  preflightCache.value = null;
  preflightCache.expiresAt = 0;
  preflightCache.pending = null;
  preflightCache.serviceRef = null;
}

module.exports = {
  analyzeImageFile,
  extractTextFromImage,
  flushAllEmbeddings,
  resetSingletons
};
