const fs = require('fs').promises;
const path = require('path');
const sharp = require('sharp');
const { getOllamaVisionModel, loadOllamaConfig, getOllama } = require('../ollamaUtils');
const { buildOllamaOptions } = require('../services/PerformanceService');
const { globalDeduplicator } = require('../utils/llmOptimization');
const { generateWithRetry } = require('../utils/ollamaApiRetry');
const { extractAndParseJSON } = require('../utils/jsonRepair');
const {
  AI_DEFAULTS,
  SUPPORTED_IMAGE_EXTENSIONS,
  PROCESSING_LIMITS
} = require('../../shared/constants');
const { TRUNCATION, THRESHOLDS } = require('../../shared/performanceConstants');
const { normalizeAnalysisResult } = require('./utils');
const {
  getIntelligentCategory: getIntelligentImageCategory,
  getIntelligentKeywords: getIntelligentImageKeywords,
  safeSuggestedName
} = require('./fallbackUtils');
const { container, ServiceIds } = require('../services/ServiceContainer');
const FolderMatchingService = require('../services/FolderMatchingService');
const embeddingQueue = require('./embeddingQueue');
const { logger } = require('../../shared/logger');
logger.setContext('OllamaImageAnalysis');
let chromaDbSingleton = null;
let folderMatcherSingleton = null;

// In-memory cache for image analysis keyed by path|size|mtimeMs
const imageAnalysisCache = new Map();
const MAX_IMAGE_CACHE = 300;
const IMAGE_SIGNATURE_VERSION = 'v2';
function setImageCache(signature, value) {
  if (!signature) return;
  imageAnalysisCache.set(signature, value);
  if (imageAnalysisCache.size > MAX_IMAGE_CACHE) {
    const first = imageAnalysisCache.keys().next().value;
    imageAnalysisCache.delete(first);
  }
}

// App configuration for image analysis - Optimized for speed
const AppConfig = {
  ai: {
    imageAnalysis: {
      defaultModel: AI_DEFAULTS.IMAGE.MODEL,
      defaultHost: AI_DEFAULTS.IMAGE.HOST,
      // Keep image analysis aligned with global processing timeouts so the renderer lock
      // doesn't get "stuck" for minutes when vision calls hang.
      timeout: PROCESSING_LIMITS.ANALYSIS_TIMEOUT,
      temperature: AI_DEFAULTS.IMAGE.TEMPERATURE,
      maxTokens: AI_DEFAULTS.IMAGE.MAX_TOKENS
    }
  }
};

async function analyzeImageWithOllama(imageBase64, originalFileName, smartFolders = []) {
  try {
    logger.info(`Analyzing image content with Ollama`, {
      model: AppConfig.ai.imageAnalysis.defaultModel
    });

    // Build folder categories string for the prompt (include descriptions)
    let folderCategoriesStr = '';
    if (smartFolders && smartFolders.length > 0) {
      const validFolders = smartFolders
        .filter((f) => f && typeof f.name === 'string' && f.name.trim().length > 0)
        .slice(0, TRUNCATION.FOLDERS_DISPLAY)
        .map((f) => ({
          name: f.name.trim().slice(0, TRUNCATION.NAME_MAX),
          description: (f.description || '').trim().slice(0, TRUNCATION.DESCRIPTION_MAX)
        }));
      if (validFolders.length > 0) {
        const folderListDetailed = validFolders
          .map((f, i) => `${i + 1}. "${f.name}" — ${f.description || 'no description provided'}`)
          .join('\n');

        folderCategoriesStr = `\n\nAVAILABLE SMART FOLDERS (name — description):\n${folderListDetailed}\n\nSELECTION RULES (CRITICAL):\n- Choose the category by comparing the IMAGE CONTENT to the folder DESCRIPTIONS above.\n- Output the category EXACTLY as one of the folder names above (verbatim).\n- Do NOT invent new categories. If unsure, choose the closest match by description or use the first folder as a fallback.`;
      }
    }

    const prompt = `You are an expert image analyzer for an automated file organization system. Analyze this image named "${originalFileName}" and extract structured information.

Your response should be a JSON object with the following fields:
- date (if there's a visible date in the image, in YYYY-MM-DD format)
- project (a short, 2-5 word project name or main subject based on image content)
- purpose (a concise, 5-10 word description of what this image shows or represents)
- category (most appropriate category for organizing this file; must be one of the folder names above)${folderCategoriesStr}
- keywords (an array of 3-7 relevant keywords describing the image content)
- confidence (a number from 60-100 indicating analysis confidence)
- content_type (e.g., 'people', 'landscape', 'text_document', 'interface', 'object', 'animal', 'food', 'vehicle', 'architecture')
- has_text (boolean indicating if there's readable text in the image)
- colors (array of 2-4 dominant colors in the image)
- suggestedName (descriptive name based on image content, underscores, max 50 chars)

If you cannot determine a field, omit it from the JSON. Do not make up information. The output MUST be a valid JSON object.

Analyze this image:`;

    const cfg = await loadOllamaConfig();
    const modelToUse =
      getOllamaVisionModel() || cfg.selectedVisionModel || AppConfig.ai.imageAnalysis.defaultModel;

    // Use deduplicator to prevent duplicate LLM calls for identical images
    // FIX: Handle case where smartFolders is explicitly passed as undefined
    const safeFolders = Array.isArray(smartFolders) ? smartFolders : [];
    const deduplicationKey = globalDeduplicator.generateKey({
      image: imageBase64.slice(0, TRUNCATION.CACHE_SIGNATURE), // Use first chars as signature
      model: modelToUse,
      fileName: originalFileName,
      folders: safeFolders.map((f) => f.name).join(',')
    });

    const client = await getOllama();
    const perfOptions = await buildOllamaOptions('vision');

    // IMPORTANT: Vision model calls can hang indefinitely if the model/server gets stuck.
    // Enforce a hard timeout and abort the underlying request (supported by Ollama client).
    const abortController = new AbortController();
    let timeoutId = null;
    const timeoutMs = Number(AppConfig.ai.imageAnalysis.timeout) || 60000;
    const startedAt = Date.now();

    const response = await (async () => {
      const timeoutPromise = new Promise((_, reject) => {
        timeoutId = setTimeout(() => {
          try {
            abortController.abort();
          } catch {
            // ignore
          }
          reject(new Error(`Image analysis timeout after ${timeoutMs}ms`));
        }, timeoutMs);

        // Ensure timers don't keep the process alive
        if (timeoutId && typeof timeoutId.unref === 'function') {
          timeoutId.unref();
        }
      });

      const generatePromise = globalDeduplicator.deduplicate(deduplicationKey, () =>
        generateWithRetry(
          client,
          {
            model: modelToUse,
            prompt,
            images: [imageBase64],
            options: {
              temperature: AppConfig.ai.imageAnalysis.temperature,
              num_predict: AppConfig.ai.imageAnalysis.maxTokens,
              ...perfOptions
            },
            format: 'json',
            signal: abortController.signal
          },
          {
            operation: `Image analysis for ${originalFileName}`,
            maxRetries: 3,
            initialDelay: 1000,
            maxDelay: 4000
          }
        )
      );

      try {
        return await Promise.race([generatePromise, timeoutPromise]);
      } finally {
        if (timeoutId) clearTimeout(timeoutId);
      }
    })();

    logger.info('[IMAGE-ANALYSIS] Ollama vision request completed', {
      fileName: originalFileName,
      model: modelToUse,
      elapsedMs: Date.now() - startedAt
    });

    if (response.response) {
      try {
        // Use robust JSON extraction with repair for malformed LLM responses
        const parsedJson = extractAndParseJSON(response.response, null);

        if (!parsedJson || typeof parsedJson !== 'object') {
          logger.warn('[IMAGE-ANALYSIS] JSON extraction failed', {
            responseLength: response.response.length,
            responsePreview: response.response.substring(0, 500)
          });
          throw new Error('Failed to parse image analysis JSON from Ollama');
        }

        // Validate and structure the date
        if (parsedJson.date) {
          try {
            parsedJson.date = new Date(parsedJson.date).toISOString().split('T')[0];
          } catch (e) {
            delete parsedJson.date;
            logger.warn('Ollama returned an invalid date for image, omitting.');
          }
        }

        // Ensure array fields are initialized if undefined
        const finalKeywords = Array.isArray(parsedJson.keywords) ? parsedJson.keywords : [];
        const finalColors = Array.isArray(parsedJson.colors) ? parsedJson.colors : [];

        // Ensure confidence is a reasonable number
        // MEDIUM PRIORITY FIX (MED-10): Use fixed default instead of random value
        if (!parsedJson.confidence || parsedJson.confidence < 60 || parsedJson.confidence > 100) {
          parsedJson.confidence = 75; // Fixed default when Ollama returns invalid confidence
          logger.debug('[IMAGE-ANALYSIS] Invalid confidence from Ollama, using default: 75');
        }

        return {
          ...parsedJson,
          keywords: finalKeywords,
          colors: finalColors,
          has_text: Boolean(parsedJson.has_text)
        };
      } catch (e) {
        logger.error('Error parsing Ollama JSON response for image', {
          error: e.message
        });
        return {
          error: 'Failed to parse image analysis from Ollama.',
          keywords: [],
          confidence: 65
        };
      }
    }

    return {
      error: 'No content in Ollama response for image',
      keywords: [],
      confidence: 60
    };
  } catch (error) {
    logger.error('Error calling Ollama API for image', {
      error: error.message
    });

    if (
      error?.name === 'AbortError' ||
      String(error?.message || '')
        .toLowerCase()
        .includes('aborted')
    ) {
      return {
        error:
          'Image analysis was aborted (timeout or cancellation). Try again or switch to a smaller/faster vision model.',
        keywords: [],
        confidence: 0
      };
    }

    // Specific handling for zero-length image error
    if (error.message.includes('zero length image')) {
      return {
        error: 'Image is empty or corrupted - cannot analyze zero-length image',
        keywords: [],
        confidence: 0
      };
    }
    // Guidance for vision model input failures
    if (error.message.includes('unable to make llava embedding')) {
      return {
        error:
          'Unsupported image format or dimensions for vision model. Convert to PNG/JPG and keep under ~2048px on the longest side.',
        keywords: [],
        confidence: 0
      };
    }

    return {
      error: `Ollama API error for image: ${error.message}`,
      keywords: [],
      confidence: 60
    };
  }
}

// ============================================================================
// Helper Functions - Extracted for readability and maintainability
// ============================================================================

/**
 * Create a fallback analysis result when AI analysis is unavailable
 * @param {string} fileName - Original file name
 * @param {string} fileExtension - File extension
 * @param {string} reason - Reason for fallback
 * @param {number} confidence - Confidence score (default: 60)
 * @returns {Object} Fallback analysis result
 */
function createFallbackResult(fileName, fileExtension, reason, confidence = 60) {
  const intelligentCategory = getIntelligentImageCategory(fileName, fileExtension);
  const intelligentKeywords = getIntelligentImageKeywords(fileName, fileExtension);
  return {
    purpose: `Image (fallback - ${reason})`,
    project: fileName.replace(fileExtension, ''),
    category: intelligentCategory,
    date: new Date().toISOString().split('T')[0],
    keywords: intelligentKeywords,
    confidence,
    suggestedName: safeSuggestedName(fileName, fileExtension),
    extractionMethod: 'filename_fallback',
    fallbackReason: reason
  };
}

/**
 * Extract EXIF date from image metadata
 * @param {Buffer} imageBuffer - Image buffer
 * @returns {Promise<string|null>} EXIF date in YYYY-MM-DD format or null
 */
async function extractExifDate(imageBuffer) {
  try {
    const meta = await sharp(imageBuffer).metadata();
    if (meta && meta.exif) {
      const exif = require('exif-reader')(meta.exif);
      if (exif && (exif.exif || exif.image)) {
        const dateStr = exif.exif?.DateTimeOriginal || exif.image?.ModifyDate;
        if (dateStr) {
          const parts = dateStr.split(' ')[0].replace(/:/g, '-');
          if (parts.match(/^\d{4}-\d{2}-\d{2}$/)) {
            logger.debug(`[IMAGE] Extracted EXIF date: ${parts}`);
            return parts;
          }
        }
      }
    }
  } catch (exifErr) {
    // Non-fatal if metadata extraction fails
    logger.debug('[IMAGE] EXIF extraction failed:', exifErr.message);
  }
  return null;
}

/**
 * Preprocess image for vision model compatibility
 * Converts unsupported formats to PNG and resizes large images
 * @param {Buffer} imageBuffer - Original image buffer
 * @param {string} fileExtension - File extension
 * @returns {Promise<Buffer>} Processed image buffer
 */
async function preprocessImageBuffer(imageBuffer, fileExtension) {
  const needsFormatConversion = ['.svg', '.tiff', '.tif', '.bmp', '.gif', '.webp'].includes(
    fileExtension
  );
  const maxDimension = 1536;

  let meta = null;
  try {
    meta = await sharp(imageBuffer).metadata();
  } catch (metaErr) {
    logger.debug('[IMAGE] Metadata extraction failed:', metaErr.message);
  }

  const shouldResize =
    meta && (Number(meta.width) > maxDimension || Number(meta.height) > maxDimension);

  if (!needsFormatConversion && !shouldResize) {
    return imageBuffer;
  }

  let transformer = sharp(imageBuffer);
  if (shouldResize) {
    const resizeOptions = { fit: 'inside', withoutEnlargement: true };
    if (meta && meta.width && meta.height) {
      if (meta.width >= meta.height) resizeOptions.width = maxDimension;
      else resizeOptions.height = maxDimension;
    } else {
      resizeOptions.width = maxDimension;
    }
    transformer = transformer.resize(resizeOptions);
  }
  return transformer.png({ compressionLevel: 5 }).toBuffer();
}

/**
 * Apply semantic folder matching using ChromaDB embeddings
 * @param {Object} analysis - Current analysis result
 * @param {string} filePath - File path for embedding
 * @param {Array} smartFolders - Available smart folders
 * @returns {Promise<Object>} Analysis with folder matching applied
 */
async function applySemanticFolderMatching(analysis, filePath, smartFolders) {
  const chromaDb = container.tryResolve(ServiceIds.CHROMA_DB);
  if (chromaDb !== chromaDbSingleton) {
    chromaDbSingleton = chromaDb;
  }

  if (!chromaDb) {
    logger.warn('[IMAGE] ChromaDB not available, skipping semantic folder refinement');
    return analysis;
  }

  const folderMatcher =
    folderMatcherSingleton || (folderMatcherSingleton = new FolderMatchingService(chromaDb));

  // Validate folder matcher has required methods
  const hasRequiredMethods =
    folderMatcher &&
    typeof folderMatcher === 'object' &&
    typeof folderMatcher.initialize === 'function' &&
    typeof folderMatcher.batchUpsertFolders === 'function' &&
    typeof folderMatcher.embedText === 'function' &&
    typeof folderMatcher.matchVectorToFolders === 'function';

  if (!hasRequiredMethods) {
    logger.warn('[IMAGE] FolderMatcher invalid or missing required methods');
    return analysis;
  }

  // Initialize on first use
  if (!folderMatcher.embeddingCache?.initialized) {
    try {
      await folderMatcher.initialize();
    } catch (initError) {
      logger.warn('[IMAGE] FolderMatcher initialization error:', initError.message);
      return analysis;
    }
  }

  // Upsert smart folders
  if (smartFolders && Array.isArray(smartFolders) && smartFolders.length > 0) {
    try {
      const validFolders = smartFolders.filter(
        (f) => f && typeof f === 'object' && (f.name || f.id || f.path)
      );
      if (validFolders.length > 0) {
        await folderMatcher.batchUpsertFolders(validFolders);
      }
    } catch (upsertError) {
      logger.warn('[IMAGE] Folder embedding upsert error:', upsertError.message);
    }
  }

  // Build summary for matching
  const summary = [
    analysis.project,
    analysis.purpose,
    (analysis.keywords || []).join(' '),
    analysis.content_type || ''
  ]
    .filter(Boolean)
    .join('\n');

  if (!summary || summary.trim().length === 0) {
    logger.debug('[IMAGE] Empty summary, skipping folder matching');
    return analysis;
  }

  try {
    const chromaDbService = container.tryResolve(ServiceIds.CHROMA_DB);
    if (chromaDbService) {
      await chromaDbService.initialize();
    }

    const { vector, model } = await folderMatcher.embedText(summary);
    const candidates = await folderMatcher.matchVectorToFolders(vector, 5);

    // Queue embedding for batch persistence
    embeddingQueue.enqueue({
      id: `image:${filePath}`,
      vector,
      model,
      meta: { path: filePath },
      updatedAt: new Date().toISOString()
    });

    if (Array.isArray(candidates) && candidates.length > 0) {
      const top = candidates[0];
      if (top && typeof top === 'object' && typeof top.score === 'number' && top.name) {
        if (top.score >= THRESHOLDS.FOLDER_MATCH_CONFIDENCE) {
          analysis.category = top.name;
          analysis.suggestedFolder = top.name;
          analysis.destinationFolder = top.path || top.name;
          logger.debug('[IMAGE] Folder match applied', {
            category: top.name,
            score: top.score
          });
        }
        analysis.folderMatchCandidates = candidates;
      }
    }
  } catch (matchError) {
    logger.warn('[IMAGE] Folder matching error:', matchError.message);
  }

  return analysis;
}

// ============================================================================
// Main Analysis Function
// ============================================================================

async function analyzeImageFile(filePath, smartFolders = []) {
  logger.info(`Analyzing image file`, { path: filePath });
  const fileExtension = path.extname(filePath).toLowerCase();
  const fileName = path.basename(filePath);
  const smartFolderSig = Array.isArray(smartFolders)
    ? smartFolders
        .map((f) => f?.name || '')
        .filter(Boolean)
        .sort()
        .join('|')
    : '';

  // Check if file extension is supported (include SVG by rasterizing via sharp)
  const supportedExtensions = SUPPORTED_IMAGE_EXTENSIONS;
  if (!supportedExtensions.includes(fileExtension)) {
    return {
      error: `Unsupported image format: ${fileExtension}`,
      category: 'unsupported',
      keywords: [],
      confidence: 0,
      suggestedName:
        fileName.replace(fileExtension, '').replace(/[^a-zA-Z0-9_-]/g, '_') + fileExtension
    };
  }

  // Fixed: Proactive graceful degradation - check Ollama availability before processing
  const ModelVerifier = require('../services/ModelVerifier');
  const modelVerifier = new ModelVerifier();

  try {
    const connectionCheck = await modelVerifier.checkOllamaConnection();
    if (!connectionCheck.connected) {
      logger.warn('[ANALYSIS-FALLBACK] Ollama unavailable, using filename-based analysis', {
        fileName,
        error: connectionCheck.error
      });
      return createFallbackResult(fileName, fileExtension, connectionCheck.error, 60);
    }
  } catch (error) {
    logger.error('[IMAGE] Pre-flight verification failed', {
      error: error.message
    });
    return createFallbackResult(fileName, fileExtension, error.message, 55);
  }

  try {
    // Resolve vision model for cache signatures
    let visionModelName = AppConfig.ai.imageAnalysis.defaultModel;
    try {
      const cfgModel = await loadOllamaConfig();
      visionModelName =
        getOllamaVisionModel() ||
        cfgModel.selectedVisionModel ||
        AppConfig.ai.imageAnalysis.defaultModel;
    } catch (err) {
      logger.debug('[IMAGE] Config load failed, using default model:', err.message);
      visionModelName = AppConfig.ai.imageAnalysis.defaultModel;
    }

    // First, check if file exists and has content
    let stats;
    try {
      stats = await fs.stat(filePath);
    } catch (statError) {
      // Handle ENOENT and other file access errors gracefully
      if (statError.code === 'ENOENT') {
        logger.error(`Image file does not exist`, { path: filePath });
        return {
          error: 'Image file not found (ENOENT)',
          category: 'error',
          keywords: [],
          confidence: 0
        };
      }
      throw statError; // Re-throw other errors
    }

    if (stats.size === 0) {
      logger.error(`Image file is empty`, { path: filePath });
      return {
        error: 'Image file is empty (0 bytes)',
        category: 'error',
        keywords: [],
        confidence: 0
      };
    }

    logger.debug(`Image file size`, { bytes: stats.size });

    // Handle TOCTOU race condition - file could be deleted between stat() and readFile()
    let imageBuffer;
    try {
      imageBuffer = await fs.readFile(filePath);
    } catch (readError) {
      if (readError.code === 'ENOENT') {
        logger.error(`Image file disappeared during read`, { path: filePath });
        return {
          error: 'Image file was deleted during analysis (TOCTOU)',
          category: 'error',
          keywords: [],
          confidence: 0
        };
      }
      throw readError; // Re-throw other errors
    }

    // Cache quick path: signature based on file stats
    const signature = `${IMAGE_SIGNATURE_VERSION}|${visionModelName}|${smartFolderSig}|${filePath}|${stats.size}|${stats.mtimeMs}`;
    if (imageAnalysisCache.has(signature)) {
      return imageAnalysisCache.get(signature);
    }

    // Extract EXIF date and preprocess image using helpers
    const exifDate = await extractExifDate(imageBuffer);
    try {
      imageBuffer = await preprocessImageBuffer(imageBuffer, fileExtension);
    } catch (preErr) {
      logger.error(`Failed to pre-process image for analysis`, {
        path: filePath,
        error: preErr.message
      });
    }

    // Validate buffer is not empty
    if (imageBuffer.length === 0) {
      logger.error(`Image buffer is empty after reading`, { path: filePath });
      return {
        error: 'Image buffer is empty after reading',
        category: 'error',
        keywords: [],
        confidence: 0
      };
    }

    logger.debug(`Image buffer size`, { bytes: imageBuffer.length });
    const imageBase64 = imageBuffer.toString('base64');

    // Validate base64 encoding
    if (!imageBase64 || imageBase64.length === 0) {
      logger.error(`Image base64 encoding failed`, { path: filePath });
      return {
        error: 'Image base64 encoding failed',
        category: 'error',
        keywords: [],
        confidence: 0
      };
    }

    logger.debug(`Base64 length`, { chars: imageBase64.length });

    // Analyze with Ollama
    let analysis;
    try {
      analysis = await analyzeImageWithOllama(imageBase64, fileName, smartFolders);

      // Merge EXIF date if available and Ollama didn't return a valid date (or override it?)
      // The plan says "populate the date field without asking LLM".
      // We'll trust EXIF over LLM hallucination if EXIF is present.
      if (exifDate) {
        if (!analysis) analysis = {};
        analysis.date = exifDate;
      }
    } catch (error) {
      logger.error('[IMAGE] Error calling analyzeImageWithOllama', {
        error: error.message,
        filePath
      });
      const fallback = createFallbackResult(fileName, fileExtension, 'analysis error', 55);
      fallback.error = error.message;
      return fallback;
    }

    // Semantic folder refinement using embeddings (delegated to helper)
    try {
      await applySemanticFolderMatching(analysis, filePath, smartFolders);
    } catch (error) {
      logger.warn('[IMAGE] Unexpected error in semantic folder refinement:', {
        error: error?.message,
        filePath
      });
    }

    // Ensure analysis is defined before checking properties
    if (!analysis) {
      logger.warn('[IMAGE] analyzeImageWithOllama returned undefined', {
        filePath
      });
      const result = createFallbackResult(fileName, fileExtension, 'undefined result');
      result.error = 'Ollama image analysis returned undefined';
      try {
        setImageCache(signature, result);
      } catch {
        // Non-fatal if caching fails
      }
      return result;
    }

    if (analysis && !analysis.error) {
      // Ensure the original file extension is preserved in suggestedName
      let finalSuggestedName = analysis.suggestedName;
      if (finalSuggestedName && fileExtension) {
        const suggestedExt = path.extname(finalSuggestedName);
        if (!suggestedExt) {
          finalSuggestedName = finalSuggestedName + fileExtension;
        }
      }
      const normalized = normalizeAnalysisResult(
        {
          ...analysis,
          content_type: analysis.content_type || 'unknown',
          suggestedName: finalSuggestedName || safeSuggestedName(fileName, fileExtension)
        },
        { category: 'image', keywords: [] }
      );
      try {
        setImageCache(signature, normalized);
      } catch {
        // Non-fatal if caching fails
      }
      return normalized;
    }

    // Fallback analysis if Ollama fails
    const result = createFallbackResult(fileName, fileExtension, 'Ollama failed');
    // Preserve keywords from partial analysis if available
    if (Array.isArray(analysis.keywords)) {
      result.keywords = analysis.keywords;
    }
    result.error = analysis?.error || 'Ollama image analysis failed.';
    try {
      setImageCache(signature, result);
    } catch {
      // Non-fatal if caching fails
    }
    return result;
  } catch (error) {
    logger.error('Error processing image', { filePath, error: error.message });
    return {
      error: `Failed to process image: ${error.message}`,
      category: 'error',
      project: fileName,
      keywords: [],
      confidence: 50
    };
  }
}

// OCR capability using Ollama for text extraction from images
async function extractTextFromImage(filePath) {
  try {
    const imageBuffer = await fs.readFile(filePath);
    const imageBase64 = imageBuffer.toString('base64');

    const prompt = `Extract all readable text from this image. Return only the text content, maintaining the original structure and formatting as much as possible. If no text is found, return "NO_TEXT_FOUND".`;

    const cfg2 = await loadOllamaConfig();
    const modelToUse2 =
      getOllamaVisionModel() || cfg2.selectedVisionModel || AppConfig.ai.imageAnalysis.defaultModel;
    const client2 = await getOllama();
    const response = await generateWithRetry(
      client2,
      {
        model: modelToUse2,
        prompt,
        images: [imageBase64],
        options: {
          temperature: 0.1, // Lower temperature for text extraction
          num_predict: 2000
        }
      },
      {
        operation: `Text extraction from image ${path.basename(filePath)}`,
        maxRetries: 3,
        initialDelay: 1000,
        maxDelay: 4000
      }
    );

    if (response.response && response.response.trim() !== 'NO_TEXT_FOUND') {
      return response.response.trim();
    }

    return null;
  } catch (error) {
    logger.error('Error extracting text from image', { error: error.message });
    return null;
  }
}

// Fallback image helpers sourced from fallbackUtils

/**
 * Force flush the embedding queue (useful for cleanup or end of batch)
 */
async function flushAllEmbeddings() {
  await embeddingQueue.flush();
}

/**
 * Reset module singletons and caches
 * Useful for hot reload, testing, or reconnecting to services
 */
function resetSingletons() {
  chromaDbSingleton = null;
  folderMatcherSingleton = null;
  imageAnalysisCache.clear();
}

module.exports = {
  analyzeImageFile,
  extractTextFromImage,
  flushAllEmbeddings,
  resetSingletons
};
