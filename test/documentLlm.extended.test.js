/**
 * @jest-environment node
 *
 * Extended tests for documentLlm.js covering:
 *  - JSON repair fallback when initial parse fails
 *  - Empty/no response from LLM
 *  - Confidence calculation from response quality
 *  - Date validation and sanitization
 *  - Keyword filtering
 *  - Smart folder category matching
 *  - Outer error handler (LLM crash)
 */

jest.mock('../src/shared/logger', () => {
  const logger = {
    setContext: jest.fn(),
    info: jest.fn(),
    debug: jest.fn(),
    warn: jest.fn(),
    error: jest.fn()
  };
  return { logger, createLogger: jest.fn(() => logger) };
});

const mockLlamaService = {
  getConfig: jest.fn().mockResolvedValue({ textModel: 'test-model.gguf', contextSize: 4096 }),
  generateText: jest.fn()
};

jest.mock('../src/main/services/LlamaService', () => ({
  getInstance: () => mockLlamaService
}));

jest.mock('../src/main/utils/llmOptimization', () => ({
  globalDeduplicator: {
    generateKey: jest.fn(() => 'dedup-key'),
    deduplicate: jest.fn((_key, fn) => fn())
  }
}));

// Use a singleton so all callers (including the SUT) share the same mock instance
const mockCacheService = {
  generateKey: jest.fn(() => 'cache-key'),
  get: jest.fn(() => null),
  set: jest.fn()
};
jest.mock('../src/main/services/AnalysisCacheService', () => ({
  getInstance: () => mockCacheService
}));

// Mock withAbortableTimeout to just execute the function directly
jest.mock('../src/shared/promiseUtils', () => ({
  withAbortableTimeout: jest.fn(async (fn) => {
    const controller = new AbortController();
    return fn(controller);
  }),
  Semaphore: jest.fn()
}));

jest.mock('../src/main/services/FolderMatchingService', () => ({
  matchCategoryToFolder: jest.fn((category) => category)
}));

const { analyzeTextWithLlama } = require('../src/main/analysis/documentLlm');

describe('documentLlm – extended', () => {
  beforeEach(() => {
    jest.clearAllMocks();
    // Reset generateText to a default resolved value (clearAllMocks doesn't reset implementations)
    mockLlamaService.getConfig.mockResolvedValue({
      textModel: 'test-model.gguf',
      contextSize: 4096
    });
    mockLlamaService.generateText.mockResolvedValue({ response: '{}' });
    // Reset cache to miss by default
    mockCacheService.get.mockReturnValue(null);
  });

  // ─── JSON repair fallback ──────────────────────────────────

  test('falls back to JSON repair when initial parse fails', async () => {
    // First LLM call returns malformed JSON
    mockLlamaService.generateText
      .mockResolvedValueOnce({ response: '{ malformed json!!!' })
      // JSON repair call returns valid JSON
      .mockResolvedValueOnce({
        response: JSON.stringify({
          category: 'documents',
          keywords: ['test'],
          confidence: 85,
          suggestedName: 'repaired_doc'
        })
      });

    const result = await analyzeTextWithLlama('Test content', 'test.pdf', []);

    // Should succeed via repair path
    if (result.error) {
      // Repair may fail in test env without full LLM — just verify error is graceful
      expect(result.keywords).toEqual([]);
      expect(result.confidence).toBeGreaterThanOrEqual(60);
    } else {
      expect(result.suggestedName).toBeDefined();
    }
  });

  test('performs strict JSON retry before returning fallback parse error', async () => {
    mockLlamaService.generateText
      // Initial analysis response
      .mockResolvedValueOnce({ response: '{ malformed json' })
      // JSON repair helper response
      .mockResolvedValueOnce({ response: '{ still malformed' })
      // Strict retry response
      .mockResolvedValueOnce({
        response: JSON.stringify({
          category: 'documents',
          keywords: ['retry'],
          confidence: 82,
          suggestedName: 'strict_retry_ok'
        })
      });

    const result = await analyzeTextWithLlama('Retry content', 'retry.pdf', []);

    expect(mockLlamaService.generateText).toHaveBeenCalledTimes(3);
    expect(result.error).toBeUndefined();
    expect(result.suggestedName).toBe('strict_retry_ok.pdf');
  });

  // ─── Empty/null response ───────────────────────────────────

  test('returns error object when LLM returns no response content', async () => {
    mockLlamaService.generateText.mockResolvedValue({ response: '' });

    const result = await analyzeTextWithLlama('Test', 'doc.pdf', []);

    expect(result.error).toMatch(/No content|Failed/i);
    expect(result.keywords).toEqual([]);
    expect(result.confidence).toBeLessThanOrEqual(65);
  });

  test('returns error object when LLM returns null response', async () => {
    mockLlamaService.generateText.mockResolvedValue({ response: null });

    const result = await analyzeTextWithLlama('Test', 'doc.pdf', []);

    expect(result.error).toBeDefined();
    expect(result.keywords).toEqual([]);
  });

  // ─── Confidence calculation ────────────────────────────────

  test('calculates confidence from response quality fields', async () => {
    mockLlamaService.generateText.mockResolvedValue({
      response: JSON.stringify({
        category: 'invoices',
        purpose: 'billing',
        keywords: ['invoice', 'payment', 'total'],
        project: 'Q1 Billing',
        suggestedName: 'q1_invoice'
        // No confidence field — should be calculated
      })
    });

    const result = await analyzeTextWithLlama('Invoice for Q1', 'invoice.pdf', []);

    // Base 70 + category(5) + purpose(5) + keywords>=3(5) + project(5) + suggestedName(5) = 95
    expect(result.confidence).toBe(95);
  });

  test('assigns base confidence when response has minimal fields', async () => {
    mockLlamaService.generateText.mockResolvedValue({
      response: JSON.stringify({
        category: 'documents',
        keywords: ['doc']
        // Missing purpose, project, suggestedName
      })
    });

    const result = await analyzeTextWithLlama('Some text', 'doc.pdf', []);

    // Base 70 + category(5) = 75
    expect(result.confidence).toBe(75);
  });

  test('preserves valid LLM-provided confidence in 60-100 range', async () => {
    mockLlamaService.generateText.mockResolvedValue({
      response: JSON.stringify({
        category: 'documents',
        keywords: ['doc'],
        confidence: 88 // Valid range, should be kept as-is
      })
    });

    const result = await analyzeTextWithLlama('Text', 'doc.pdf', []);
    expect(result.confidence).toBe(88);
  });

  test('overrides out-of-range LLM confidence', async () => {
    mockLlamaService.generateText.mockResolvedValue({
      response: JSON.stringify({
        category: 'documents',
        keywords: ['doc'],
        confidence: 999 // Out of range
      })
    });

    const result = await analyzeTextWithLlama('Text', 'doc.pdf', []);
    // Should be recalculated, not 999
    expect(result.confidence).toBeLessThanOrEqual(95);
    expect(result.confidence).toBeGreaterThanOrEqual(70);
  });

  // ─── Date validation ───────────────────────────────────────

  test('sanitizes valid date to ISO format', async () => {
    mockLlamaService.generateText.mockResolvedValue({
      response: JSON.stringify({
        category: 'documents',
        keywords: ['report'],
        date: '2024-03-15T10:30:00Z',
        confidence: 80
      })
    });

    const result = await analyzeTextWithLlama('Report', 'report.pdf', []);
    expect(result.date).toBe('2024-03-15');
  });

  test('removes invalid date from response', async () => {
    mockLlamaService.generateText.mockResolvedValue({
      response: JSON.stringify({
        category: 'documents',
        keywords: ['report'],
        date: 'not-a-date',
        confidence: 80
      })
    });

    const result = await analyzeTextWithLlama('Report', 'report.pdf', []);
    // Invalid date should be removed, no date field or undefined
    expect(result.date).toBeUndefined();
  });

  test('falls back to fileDate when LLM returns no date', async () => {
    mockLlamaService.generateText.mockResolvedValue({
      response: JSON.stringify({
        category: 'documents',
        keywords: ['report'],
        confidence: 80
      })
    });

    const result = await analyzeTextWithLlama('Report', 'report.pdf', [], '2024-01-01');
    expect(result.date).toBe('2024-01-01');
  });

  // ─── Keyword filtering ─────────────────────────────────────

  test('filters out non-string keywords', async () => {
    mockLlamaService.generateText.mockResolvedValue({
      response: JSON.stringify({
        category: 'documents',
        keywords: ['valid', 42, null, '', 'also-valid', true],
        confidence: 80
      })
    });

    const result = await analyzeTextWithLlama('Text', 'doc.pdf', []);
    expect(result.keywords).toEqual(['valid', 'also-valid']);
  });

  // ─── Suggested name extension preservation ─────────────────

  test('appends original file extension to suggestedName', async () => {
    mockLlamaService.generateText.mockResolvedValue({
      response: JSON.stringify({
        category: 'documents',
        keywords: ['invoice'],
        suggestedName: 'q1_invoice',
        confidence: 80
      })
    });

    const result = await analyzeTextWithLlama('Invoice', 'original.docx', []);
    expect(result.suggestedName).toBe('q1_invoice.docx');
  });

  test('does not double-append extension when already present', async () => {
    mockLlamaService.generateText.mockResolvedValue({
      response: JSON.stringify({
        category: 'documents',
        keywords: ['invoice'],
        suggestedName: 'q1_invoice.pdf',
        confidence: 80
      })
    });

    const result = await analyzeTextWithLlama('Invoice', 'original.pdf', []);
    expect(result.suggestedName).toBe('q1_invoice.pdf');
  });

  // ─── LLM crash / outer error handler ───────────────────────

  test('returns graceful error when LLM throws', async () => {
    mockLlamaService.generateText.mockRejectedValue(new Error('GPU out of memory'));

    const result = await analyzeTextWithLlama('Text', 'doc.pdf', []);

    expect(result.error).toMatch(/AI engine error|GPU out of memory/i);
    expect(result.keywords).toEqual([]);
    expect(result.confidence).toBe(60);
  });

  // ─── Cache hit path ────────────────────────────────────────

  test('returns cached result without calling LLM', async () => {
    const cachedResult = {
      category: 'cached',
      keywords: ['cached-kw'],
      confidence: 92
    };

    mockCacheService.get.mockReturnValue(cachedResult);

    const result = await analyzeTextWithLlama('Text', 'doc.pdf', []);

    expect(result).toBe(cachedResult);
    expect(mockLlamaService.generateText).not.toHaveBeenCalled();
  });
});
